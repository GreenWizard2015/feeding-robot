# Распознавание меток на роботе

## [Commit #1](https://github.com/GreenWizard2015/feeding-robot/tree/1b2ec16fc4f90d8788800f3333e78b221a17cdda)

Основные моменты:

- Используется очень примитивная сеть на базе архитектуры U-net. На вход поступает BGR-изображение (из предобратки, только нормирование диапазона до 0..1) размером 224*224, а на выходе получаем пиксельную карту для 8 классов/точек + 1 карта для "фона".
  
- Локализация точки из карты сегментации является весьма сложным (максимум не всегда указывает на нужную точку, особенно при масштабировании), поэтому было решено находить "центр масс" предсказаний. Так же, данный подход используется и на этапе обучения сети, что повысило точность и стабильность. Хоть dice loss уже учитывает целевую форму распределения вероятностей, но расчёт центра масс позволил убрать артефакты (особенно в точках 7 и 8, которые видны значительно реже).

- Датасет наполнялся итеративно. Сначала было размечено 10 кадров и обучена на них сеть. Затем было отобрано ещё 10 кадров на которых сеть делала заметные ошибки и сеть обучалась вновь. Сейчас датасет содержит 91 кадр. 67 из них извлечены с первого видео-файла (около 1250 кадров), НО сеть уже способна сама доразметить данные с достаточно высокой точностью.

- Использовалась примитивная, но весьма сильная, аугментация данных. Конечно, некоторые нюансы реализации являются весьма сомнительными, но даже это позволило добиться корректных предсказаний даже на видео с другой камеры (не всегда, но в основном).

Видео-файлы и обученные модели можно [скачать отдельно](https://drive.google.com/file/d/1XMTd2z23sf3oe3hz0eZJf5uFK_LfDyfE/).

Видео-демонстрация работы - [youtube](https://youtu.be/qfuOcrQkL3o).

## Adversarial loss ([commit](https://github.com/GreenWizard2015/feeding-robot/tree/8351aa58ee9fe39e845e50393654861032b813b3))

Входными данными для дискриминатора стала предсказанная карта для каждой из меток (без "не метка"), что позволило существенно упростить задачу и добиться адекватной точности дискриминатора.

Основная сеть могла подстраиваться под дискриминатор, поэтому была введена история сэмплов. С достаточно низким шансом, примеры предсказаний основной отправлялись в историю или же заменялись более старыми, из истории. Таким образом, дискриминатор постепенно обучался всё более новым деталям, которые отличают желаемые результаты (идеальные точечные гауссовые пятна) от предсказанных.

Обычно GAN-ы обучаются самостоятельно, не имея какого-либо чёткого loss-a описывающего задачу (что влечёт за собой массу проблем). В данном же случае, у нас уже есть два основных loss-a (dice и расположение центра масс), поэтому дискриминатор должен был, как минимум, не ухудшать процесс обучения (особенно при переобучении, с чем часто возникают проблемы в GAN-ах). Чтоб избежать всех этих проблем, использовались следующие приёмы:

- дискриминатор обучался только по необходимости, а не параллельно с основной сетью. Основная сеть обучалась либо N эпох, количество которых постепенно увеличивалось, либо пока дискриминатор не переставал выдавать чёткий сигнал улучшения сети (генерируемые ответы должны становиться всё более похожими на настоящие... если этого не происходит более 10 эпох, то дискриминатор, возможно, перестал их чётко различать). Это снижает риск переобучения дискриминатора и мы меньше тратим вычислительных ресурсов.
  
- оценка дискриминатора масштабируется таким образом, чтоб вносить не более 40% в значение loss-a. Большую часть времени, по сути, сеть оптимизирует целевой loss, но получая доп. обратную связь от дискриминатора. Так же, на силу этой обратной связи косвенно влияет точность дискриминатора во время обучения. Хоть такого не было замечено, но дискриминатор может перестать различать настоящие сэмплы и сгенерированные, поэтому значимость его оценки снизится.
  
- если дискриминатор начинает выдавать одно значение (переобучившись, например), то это не ухудшает обучение т. к. основной loss начинает давать более стабильный сигнал (сложнее отличить 0.123+0.01 от 0.113+0.02, чем 0.123+1 от 0.113+1).

Так же, пришлось отказаться от тестового прогона основной сети. Это, конечно, помогало отловить более удачные модели, но потребовалось бы отключать дискриминатор, а толк от этого сомнительный. Кроме того, дискриминатор обучается именно на тестовой выборки, поэтому она уже включена в дискриминатор, а значит основная сеть обучается и на тестовой выборке.

## Планы и идеи

- Автоматически разметить больше данных, используя текущую версию сети. В `CAnchorsDetector.combinedDetections` реализован прототип данного функционала, но необходимо корректно реализовать процесс обучения на сгенерированных данных. Например, учесть возможность наличия ошибок.

- Робот является жёсткой конструкцией и возможные конфигурации расположения меток ограничены ею, поэтому можно обучить модель, например, определять является ли валидным заданное расположение меток. Подобная модель сможет пригодиться не только для распознавания, как доп. loss, но и, например, при планировании движений манипулятора.
